---
title: "Multiple Testing"
author: "Kohei Kawaguchi, Hong Kong University of Science and Technology"
output: 
  ioslides_presentation:
    widescreen: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Multiple Hypotheses

- There are $M$ null hypotheses: $H_0^m$, $m = 1, \cdots, M$.
- Suppose that $H_0^m$, $m = 1, \cdots, M$ are all true.
- The piece-wise type-I error of a test: $\mathbb{P}\{\text{Reject   }H_0^m|H_0^m\}$, $m = 1, \cdots, M$. 
- Even if the piece-wise type-I error is $\alpha$, the probability that at least one of the null hypotheses is rejected is greater than $\alpha$:
$$
\begin{split}
&\mathbb{P}\{\exists m = 1, \cdots, M, H_0^m \text{ is rejected}|\forall m = 1, \cdots, M, H_0^m\} \\
&= 1 - (1 - \alpha)^M.
\end{split}
$$
- For example, if $\alpha = 0.05$ and $M = 100$, then the probability that at least one of the null hypotheses is rejected is $1 - (1 - 0.05)^{100} \approx$ `r 1 - (1 - 0.05)^{100}`.

## Family-Wise Error Rate (FWER)

- __Definition__: _Family-wise error rate_: The probability of rejecting at least one null hypotheses when all null hypotheses are true:
$$
\text{FWER} = \mathbb{P}\{\exists m = 1, \cdots, M, H_0^m \text{ is rejected}|\forall m = 1, \cdots, M, H_0^m\}.
$$

## Bonferroni Correction

- __Bonferroni correction__: The most conservative method to control FWER.
- Depends on the followig inequality:
$$
\begin{split}
&\mathbb{P}\{\exists m = 1, \cdots, M, H_0^m \text{ is rejected}|\forall m = 1, \cdots, M, H_0^m\}\\
&\leq \sum_{m=1}^M \mathbb{P}\{H_0^m \text{ is rejected}|H_0^m\} = M \alpha.
\end{split}
$$
- Therefore, for each hypothesis, if you consider a test with a significance level $\alpha/M$, then the FWER is no greater than $\alpha$.
- The above inequality holds regardless of the dependence structure among the tests.
- The Bonferrorni correction: Reject $H_0^m$ if the piece-wise p-value for $H_0^m$ is $p^m \le \alpha/M$.

## Why Bonferroni Correction is Conservative?

- Bonferrorni correction controls for the false rejection when all null hypotheses are true.
- However, in reality, only $m \le M$ null hypotheses may be true.
- Indeed, some p-values may be smaller than others.
- How about rejecting the null hypothesis with the smallest p-value first?
- Then, considering a multiple testing of smaller number of hypotheses?

## Bonferroni-Holm Correction

1. Order the p-values: $p^{(1)} \le p^{(2)} \le \cdots \le p^{(M)}$.
2. For $l = 1, \cdots, M$ (starting with the smallest p-value):
    1. If $p^{(l)} \le \alpha/(M - l + 1)$, reject $H_0^{(l)}$ and move to $l + 1$.
    2. If $p^{(l)} > \alpha/(M - l + 1)$ accept $H_0^{(l)}$ and accept the remaining null hypotheses.
- If none of the null hypotheses is rejected by the Bonferroni correction, then none of the null hypotheses is rejected by the Bonferroni-Holm correction, either.
- If a null hypothesis is rejected by the Bonferroni correction, then it is also rejected by the Bonferroni-Holm correction because $p^m \le \alpha/M \le \alpha/(M - m + 1)$.

## Bonferroni-Holm Correction

- The Bonferroni-Holm correction controls for the FWER.
- Suppose that there are $1 \le M_0\le M$ null hypotheses that are true.
- Let $l$ be the first rejected true null hypothesis in the Bonferroni-Holm correction.
- By definition, $H_0^{(1)}, \cdots, H_0^{(l-1)}$ were correctly rejected hypotheses that are false.
- Therefore, $l - 1 \le M - M_0 \Leftrightarrow 1/(M - l + 1) \le 1 / M_0$.
- Because $l$ is rejected, $p^{(l)} \le \alpha/(M - l + 1) \le \alpha/M_0$.
- Therefore, if there is at least one true hypothesis that is rejected, then its p-value is no greater than $\alpha/M_0$.

## Bonferroni-Holm Correction

$$
\begin{split}   
\text{FWER} &= \mathbb{P}\{\exists m = 1, \cdots, M_0, H_0^m \text{ is rejected}|\forall m = 1, \cdots, M_0, H_0^m\} \\
&= \mathbb{P}\left\{\bigcup_{l = 1}^{M_0} \{l \text{   is first rejected}\}||\forall m = 1, \cdots, M_0, H_0^m \right\}\\
&\le \sum_{l = 1}^{M_0} \mathbb{P}\{l \text{   is first rejected}|\forall m = 1, \cdots, M_0, H_0^m\}\\
&\le \sum_{l = 1}^{M_0} \mathbb{P}\left\{p^l \le \frac{\alpha}{M_0}|\forall m = 1, \cdots, M_0, H_0^m\right\}\\
&= \sum_{l = 1}^{M_0} \frac{\alpha}{M_0}\\
&= \alpha.
\end{split}
$$

## Remarks

- Neither Bonferroni correction or Bonferroni-Holm correction exploit the correlation among test statistics.
- We can improve the power by exploiting the correlation using a bootstrap method (Romano and Wolf, 2010).


## False Discovery Rate (FDR)

- FWER is required when making a single type-I error can cause a problem.
   - e.g. The inspection of parts of a car
   - e.g. Validation checks for identification assumptions
- In some situations, as long as there are some true rejections, it is not a problem of having a few false rejections.
   - e.g. The inspection of a car assembly line
   - e.g. Multiple conditional average treatment effects
- __Definition__: _False Discovery Rate_: The expected proportion of null hypotheses that are mistakenly rejected when all null hypotheses are true:
$$
FDR = \mathbb{E}\left\{\frac{\sum_{m = 1}^M1\{H_0^m \text{   is rejected}\}}{M} | \forall m = 1, \cdots, M, H_0^m\right\}
$$ 

## Benjamin-Hotchberg Correction

1. Order the p-values: $p^{(1)} \le p^{(2)} \le \cdots \le p^{(M)}$.
1. For $l = M, \cdots, 1$ (starting with the highest p-value):
    1. If $p^{(l)} \le \alpha l/M$, reject $H_0^{(l)}, l, \cdots, 1$.
    1. If $p^{(l)} > \alpha l/M$, accept $H_0^{(l)}$ and move to $l - 1$.

## Positive regression dependence

- If the test statistics are _independent_, then the Benjamin-Hotchberg correction controls for the FDR.
- If the test statistics are _dependent_, then the Benjamin-Hotchberg correction controls for the FDR under the _positive regression dependence_ assumption.
- __Definition__: _Increasing set__: A set $D$ is said to be increasing if $x \in D$ and $y \ge x$ implies $y \in D$.
- __Definition__: _Positive regression dependence_: For test statistics $X^m$, $m = 1, \cdots, M$, for any increasing set $D$, and for any $\mathcal{M}_0 \subset \mathcal{M}$, $\mathbb{p}\{\{X^m\} \in D| X^m = x\}$ is non-decreasing in $x$.
- If the joint distribution of the test statistics satisfies positive regression dependence for the set of true null hypothesis $\mathcal{M}_0 \subset \mathcal{M}$, then the Benjamin-Hotchberg correction controls for the FDR no greater than $\alpha M_0/M$.


## General case

- In general, the Benjamin-Hotchberg correction may not control for the FDR.
- In this case, we can use the bootstrap method to control for the FDR (Romano et al., 2008).

## References

- Benjamini, Yoav, and Yosef Hochberg (1995). "Controlling the false discovery rate: a practical and powerful approach to multiple testing." _Journal of the Royal Statistical Society. Series B (Methodological)_, 57(1), 289-300.
- Romano, J. P., and Wolf, M. (2010). "Efficient computation of adjusted p-values for resampling-based stepdown multiple testing." _Statistics and Probability Letters_, 80(9-10), 915-920.
- Romano, J. P., Shaikh, A. M., and Wolf, M. (2008). "Control of the false discovery rate under dependence using the bootstrap and subsampling." _Test_, 17(3), 417-442.
