---
title: "Regression Discontinuity"
subtitle: "Validation check"
output: 
  html_document:
    css: style_do.css
    number_sections: yes
    toc: yes
    toc_depth: '3'
    toc_float: yes
    theme: "spacelab" #"default", "bootstrap", "cerulean", "cosmo", "darkly", "flatly",
                 # "journal", "lumen", "paper", "readable", "sandstone", "simplex",
                 # "spacelab", "united", "yeti"
    highlight: "pygments" #"default", "tango", "pygments", "kate", "monochrome",
                 # "espresso", "zenburn", "haddock", "textmate"
    df_print: paged
    code_folding: show
date: 'Last update: `r Sys.Date()`'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = FALSE, echo = TRUE, fig.align = "center")
library(AppliedEconometricsCode)
library(foreach)
library(magrittr)
library(ggplot2)
library(kableExtra)
library(modelsummary)
library(rdrobust)
color_main <- scales::viridis_pal(option = "C")(1)
```

Here, we implement the estimation method for sharp regression discontinuity design using the `rdrobust` package. The functions called from `AppliedEconometricsCode` are defined in `R/functions_rdrobust.R`. Below, we generate samples based on Lee (2008), set options for the `rdrobust` package, and explain the interpretation of the results. Additionally, we conduct a density test, which is the verification process for local randomization, based on the `rddensity` package.

# Lee (2008)

# Generating simulation data

## Setting constants and parameters

First, fix the seed and set the number of individuals `N`.

```{r}
set.seed(1)
N <- 500
```

## Generating data


Next, we generate samples for the RD design by approximating the conditional expectation function of potential outcomes as in Lee (2008). Following CCT (2014) and others, the score `s` is distributed as `2*Beta(2,4) - 1`. This distribution has a longer tail on the treatment side, resulting in more observations in the control group than in the treatment group, as shown below.

```{r}
plot <- 
  data.frame(
    x = 
      c(
        -1, 
        1
      )) %>%
  ggplot(
    aes(
      x = x
      )
    ) + 
  stat_function(
    fun = 
      function(x) {
        dbeta(
          x = (x + 1)/2, 
          shape1 = 2, 
          shape2 = 4
        )
      }
    ) + 
  labs(
    y = "Density"
  ) +
  theme_classic()
plot
```

The observed outcome variable is generated by adding normal distribution noise to a fifth-degree polynomial of the score, as specified in the `specify_mu_lee.R` function. This corresponds to the data for Figure 4 (a) in Lee (2008), where the dependent variable is the vote share, and the data is fitted with a fifth-degree polynomial limited to samples with a vote margin of less than 0.99 and more than -0.99.

```{r}
plot <- 
  data.frame(
    x = 
      c(
        -1, 
        1
      )
  ) %>%
  ggplot(
    aes(
      x = x
    )
  ) + 
  stat_function(
    fun = specify_mu_lee,
    geom = "point"
  ) + 
  labs(
    y = "Regression function",
    x = "Score"
  ) +
  theme_classic()
plot
```

Generate Lee data and name it `dgp_lee`.

```{r}
dgp_lee <- 
  generate_dgp_lee(
    N = N
  )
```

As shown below, the simple average difference between the treatment and control groups is `0.63 - 0.26 = 0.37`. However, the true effect, comparing the left limit to the right limit around the score, is `0.52 - 0.48 = 0.04`, which does not match. This discrepancy arises because the difference in simple averages does not compare the vicinity around the score threshold.

```{r}
mean_y <-
  dgp_lee %>%
    dplyr::group_by(d) %>%
    dplyr::summarize(
      mean_y = mean(y)
    ) %>%
  dplyr::pull(mean_y) 
mean_y  %>%
  kbl() %>%
  kable_styling()

lim_mean_y <-
  data.frame(
    d = c(
      FALSE, 
      TRUE
    ),
    mean_y = c(
      specify_mu_lee(
        s = -1.e-30
      ),
      specify_mu_lee(
        s = 0
      )
    )
  )
lim_mean_y %>%
  kbl() %>%
  kable_styling()
```

## Density Test

The density test uses the method of Cattaneo, Jansson, and Ma (2020) to test the continuity at the threshold of the density function of the score variable. The `rddensity` package performs the test by obtaining the density function as the first derivative of the local quadratic estimation of the distribution function.

```{r}
result_density <- 
  rddensity::rddensity(
    X = dgp_lee$s
  )
summary(result_density)
```

The test statistic value is -0.2074, and the p-value is 0.8357.

The estimation results can be illustrated along with the histogram using the `rdplotdensity` function as follows.

```{r}
rddensity::rdplotdensity(
  rdd = result_density,
  X = dgp_lee$s
)
```

## Relationship Between Bandwidth and Bias-Variance

By fixing the pilot bandwidth and varying the estimation bandwidth from the MSE-optimal, both bias and (pre-bias-correction) variance change. The choice of bandwidth significantly affects the estimation results.

```{r}
result_opt <- 
  rdrobust::rdrobust(
    y = dgp_lee$y,
    x = dgp_lee$s
    )

table <- 
  data.frame(
    case = "optimal",
    absolute_bias = sum(abs(result_opt$bias)), 
    se_raw = result_opt$se[1], 
    bandwidth_h = result_opt$bws[1,1],
    bandwidth_b = result_opt$bws[2,1]
  )

result_small <- 
  rdrobust::rdrobust(
    y = dgp_lee$y,
    x = dgp_lee$s,
    h = result_opt$bws[1, 1] / 10,
    b = result_opt$bws[2, 1]
  )

table <- 
  rbind(
    table,
    data.frame(
      case = "small",
      absolute_bias = sum(abs(result_small$bias)),
      se_raw = result_small$se[1],
      bandwidth_h = result_small$bws[1, 1],
      bandwidth_b = result_small$bws[2, 1]
    )
  )

result_large <- 
  rdrobust::rdrobust(
    y = dgp_lee$y,
    x = dgp_lee$s,
    h = result_opt$bws[1, 1] * 4,
    b = result_opt$bws[2, 1]
  )
table <- 
  rbind(
    table,
    data.frame(
      case = "large",
      absolute_bias = sum(abs(result_large$bias)),
      se_raw = result_large$se[1],
      bandwidth_h = result_large$bws[1, 1],
      bandwidth_b = result_large$bws[2, 1]
    )
  )
  
table %>%
  kbl() %>%
  kable_styling()
```

As the results above show, when the bandwidth is narrower than the optimal bandwidth, the bias decreases while the standard error increases. Conversely, when the bandwidth is wider than the optimal bandwidth, the standard error decreases while the bias also decreases.

